\chapter{Computer Architecture recalls}

\section{Computer architecture}
It mainly consists of the following elements:
\begin{itemize}
    \item Central Processing Unit (CPU)
    \item Memory Unit
    \item Input/Output Devices( Mass Storage Unit, Keyboard, Display\ldots)
\end{itemize}

Furthermore, the CPU itself consists of three main components:
\begin{itemize}
    \item Control Unit (CU)
    \item Arithmetic/Logic Unit (ALU)
    \item Registers
\end{itemize}

\subsection{Memory}

A computer's memory is where the temporary data and instructions of currently
running programs are located. A computer's memory is also known as Primary
Memory. It is the primary location the CPU uses to retrieve and process data.
It does so very frequently, so the memory must be extremely fast in storing and
retrieving data and instructions.

There are two main types of memory:

\subsubsection{Cache}

memory is usually located within the CPU itself. It runs at the same clock speed as the CPU. 

There are usually three levels of cache memory, depending on their closeness to the CPU core:

\begin{itemize}
        \item Level 1 Cache Usually in kilobytes, the fastest memory
            available, located in each CPU core
        \item Level 2 Cache Usually in megabytes, shared between all CPU cores.
        \item Level 3 Cache Usually in megabytes (Not all CPUs use L3.)
\end{itemize}

\subsubsection{ RAM} located far away from the CPU cores  is much slower than cache
memory. Accessing data from RAM addresses takes many more instructions.

For example, retrieving an instruction from the registers takes only one clock
cycle, and retrieving it from the L1 cache takes a few cycles, while retrieving
it from RAM takes around 200 cyclesa.

In the past, with 32-bit addresses, memory addresses were limited from
0x00000000 to 0xffffffff. This meant that the maximum possible RAM size was 232
bytes, which is only 4 gigabytes, at which point we run out of unique
addresses. With 64-bit addresses, the range is now up to 0xffffffffffffffff,
with a theoretical maximum RAM size of 264 bytes, which is around 18.5 exabytes
(18.5 million terabytes), so we shouldn't be running out of memory addresses
anytime soon.

When a program is run, all of its data and instructions are moved from the
storage unit to the RAM to be accessed when needed by the CPU. 
When a program is closed, its data is removed or made available to re-use from
the RAM.


\subsubsection{Memory Layout of a program}
 The actual layout of a program's in-memory image is left entirely up to the
 operating system, and often the program itself as well. This article focus on
 the concepts of code and data segments of a program and does not take any
 specific platform into account. For a running program both the machine
 instructions (program code) and data are stored in the same memory space. The
 memory is logically divided into text and data segments. Modern systems use a
 single text segment to store program instructions, but more than one segment
 for data, depending upon the storage class of the data being stored there.

 \begin{figure}
  \includegraphics[width=\linewidth]{binary/intro/images/memory-layout.png}
  \caption{Linux memory layout}
  \label{fig:linux-memory-layout}
\end{figure}

{\bf Text Segment}

text segment contains machine code of the compiled
program. The text segment of an executable object file is often read-only
segment that prevents a program from being accidentally modified.

{\bf Data Segments}

Data segment stores program data. This data could be in form of initialized or
uninitialized variables, and it could be local or global. Data segment is
further divided into four sub-data segments (initialized data segment,
uninitialized or \verb+.bss+ data segment, stack, and heap) to store variables
depending upon if they are local or global, and initialized or uninitialized.

{\bf Initialized Data or Data Segment}

Initialized data or simply data segment stores all global, static, constant,
and external variables (declared with \verb+extern+ keyword) that are initialized
beforehand.


{\bf Uninitialized Data or bss Segment}

Contrary to initialized data segment, uninitialized data or \verb+.bss+ segment stores
all uninitialized global, static, and external variables.This section occupies
no actual space in the object file; it is merely a place holder. Object file
formats distinguish between initialized and uninitialized variables for space
efficiency; uninitialized variables do not have to occupy any actual disk space
in the object file.

{\bf Stack Segment}

Stack segment is used to store all local variables and is used for passing
arguments to the functions along with the return address of the instruction
which is to be executed after the function call is over. Local variables have a
scope to the block which they are defined in; they are created when control
enters into the block. Local variables do not appear in data or bss segment.
Also all recursive function calls are added to stack. Data is added or removed
in a last-in-first-out manner to stack. When a new stack frame needs to be
added (as a result of a newly called function), the stack grows downward.

{\bf Heap Segment}
 Heap segment is also part of RAM where dynamically allocated variables are
 stored. 

The stack and heap are traditionally located at opposite ends of the process's
virtual address space.

\subsection{IO/Storage}

Finally, we have the Input/Output devices, like the keyboard, the screen, or
the long-term storage unit, also known as Secondary Memory. The processor can
access and control IO devices using Bus Interfaces, which act as 'highways' to
transfer data and addresses, using electrical charges for binary data.

Each Bus has a capacity of bits it can carry simultaneously. This usually is a multiple of 4-bits, ranging up to 128-bits. Bus interfaces are also usually used to access memory and other components outside the CPU itself. 

Unlike primary memory the storage unit stores permanent data.

The storage unit is the slowest to access. 

\section{CPU architecture}

The Central Processing Unit (CPU) is the main processing unit within a
computer. 

The CPU contains both :
\begin{itemize}
    \item the {\bf Control Unit (CU)}in charge of moving and controlling data
    \item the {\bf Arithmetic/Logic Unit (ALU)}in charge of performing various
        arithmetics and logical calculations as requested by a program through
        the assembly instructions.
\end{itemize}

The manner in which and how efficiently a CPU processes its instructions
depends on its {\bf Instruction Set Architecture (ISA)}. There are multiple
ISA's in the industry, each having its way of processing data:
\begin{itemize}
    \item {\bf RISC} architecture: ased on processing more simple instructions, which takes more cycles, but each cycle is shorter and takes less power. 
    \item {\bf CISC} architecture is based on fewer, more complex instructions, which
        can finish the requested instructions in fewer cycles, but each
        instruction takes more time and power to be processed.
\end{itemize}

\subsection{Clock Speed \& Clock Cycle}

Each CPU has a clock speed that indicates its overall speed. Every tick of the
clock runs a clock cycle that processes a basic instruction, such as fetching
an address or storing an address. Specifically, this is done by the CU or AU.

The frequency in which the cycles occur is counted is cycles per second
(Hertz). If a CPU has a speed of 3.0 GHz, it can run 3 billion cycles every
second (per core).

 \begin{figure}
  \includegraphics[width=\linewidth]{binary/intro/images/assembly_clock_cycle_0.jpg}
  \caption{Clock cycle}
  \label{fig:clock_cycle_O}
\end{figure}

Modern processors have a multi-core design, allowing them to have multiple
cycles at the same time.


\subsection{Instruction Cycle}
An Instruction Cycle is the cycle it takes the CPU to process a single machine
instruction. An instruction cycle consists of four stages:
\begin{itemize}
    \item Fetch: Takes the next instruction's address from the {\bf Instruction
        Address Register (IAR)}.
    \item Decode: Takes the instruction from the IAR, and decodes it from
        binary to see what is required to be executed.
    \item Execute: Fetch instruction operands from register/memory, and process
        the instruction in the ALU or CU.
    \item Store: Store the new value in the destination operand.
\end{itemize}

Each Instruction Cycle takes multiple clock cycles to finish, depending on the
CPU architecture and the complexity of the instruction. Once a single
instruction cycle ends, the CU increments to the next instruction and runs the
same cycle on it, and so on.

 \begin{figure}
  \includegraphics[width=\linewidth]{binary/intro/images/assembly_clock_cycle_1.jpg}
  \caption{Instruction cycle}
  \label{fig:clock_cycle_1}
\end{figure}


In the past, processors used to process instructions sequentially, so they had
to wait for one instruction to finish to start the next. On the other hand,
modern processors can process multiple instructions in parallel by having
multiple instruction/clock cycles running at the same time. This is made
possible by having a multi-thread and multi-core design.

 \begin{figure}
  \includegraphics[width=\linewidth]{binary/intro/images/assembly_clock_cycle_2.jpg}
  \caption{Multi-thread instruction cycle}
  \label{fig:clock_cycle_2}
\end{figure}

\subsection{Processor Specific}

Each processor type has a different low-level assembly language
architecture known as {\bf Instruction Set Architectures (ISA)}.

Furthermore, a single Instruction Set Architecture may have several syntax
interpretations for the same assembly code. The instruction written as
\verb+add rax, 1+ with Intel syntax, and written as \verb+addb $0x1,%rax+ with
AT\&T syntax.

As we can see, even though we can tell that both instructions are similar and
do the same thing, their syntax is different, and the locations of the source
and destination operands are swapped as well. Still, both codes assemble the
same machine code and perform the same instruction.



So, each processor type has its Instruction Set Architectures, and each
architecture can be further represented in several syntax formats.

in linux \verb+lscpu+ provide information on the the architecture:
\begin{verbatim}
$ lscpu

Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian

<SNIP>
\end{verbatim}

As we can see in the above output, the CPU architecture is \verb+x86_64+, and
supports 32-bit and 64-bit. The byte order is {\bf Little Endian}. 


\section{Instruction set architectures}
An Instruction Set Architecture (ISA) specifies the syntax and semantics of the
assembly language on each architecture. It is not just a different syntax but
is built in the core design of a processor, as it affects the way and order
instructions are executed and their level of complexity. ISA mainly consists of
the following components:

\begin{itemize}
    \item  Instructions: the instruction to be processed in the {\bf opcode
        operand\_list format}. There are usually 1,2, or 3 comma-separated
    operands \verb+add rax, 1+ \ldots)
    \item  Registers: Used to store operands, addresses, or instructions
        temporarily (\verb+rax, rsp, rip+)
    \item  Memory Addresses: The address in which data or instructions are
        stored. May point to memory or registers 
        (\verb+0xffffffffaa8a25ff, 0x44d0, $rax+)
    \item  Data Types:  The type of stored data (byte, word,\ldots).
\end{itemize}

These are the main components that distinguish different ISA's and assembly languages. 

There are two main Instruction Set Architectures that are widely used:

\begin{itemize}
    \item Complex Instruction Set Computer (CISC) - Used in Intel and AMD
        processors in most computers and servers.
    \item Reduced Instruction Set Computer (RISC) - Used in ARM and Apple
        processors, in most smartphones, and some modern laptops.
\end{itemize}

Let us see the pros and cons of each and the main differences between them.

\subsection{CISC}

The CISC architecture was one of the earliest ISA's ever developed. As its name
suggests, the CISC architecture favors more complex instructions to be run at a
time to reduce the overall number of instructions. This is done to rely as much
as possible on the CPU by combining minor instructions into more complex
instructions.

For example, suppose we were to add two registers with the \verb+add rax, rbx+
instruction. In that case, a CISC processor can do this in a single
'Fetch-Decode-Execute-Store' instruction cycle, without having to split it into
multiple instructions to fetch \verb+rax+, then fetch \verb+rbx+, then add
them, and then store them in `rax, each of which would take its own
'Fetch-Decode-Execute-Store' instruction cycle.

Two main reasons drove this:
\begin{itemize}
    \item To enable more instructions to be executed at once by designing the
        processor to run more advanced instructions in its core.
    \item In the past, memory and transistors were limited, so it was preferred
        to write shorter programs by combining multiple instructions into one.
\end{itemize}

To enable the processors to execute complex instructions, the processor's
design becomes more complicated, as it is designed to execute a vast amount of
different complex instructions, each of which has its own unit to execute it.

Furthermore, even though it takes a single instruction cycle to execute a
single instruction, as the instructions are more complex, each instruction
cycle takes more clock cycles. This fact leads to more power consumption and
heat to execute each instruction.

\subsection{RISC}

The RISC architecture favors splitting instructions into minor instructions,
and so the CPU is designed only to handle simple instructions. This is done to
relay the optimization to the software by writing the most optimized assembly
code.

For example, the same previous \verb+add r1, r2, r3+ instruction on a RISC
processor would fetch \verb+r2+, then fetch \verb+r3+, add them, and finally
store them in \verb+r1+. Every instruction of these takes an entire
'Fetch-Decode-Execute-Store' instruction cycle, which leads, as can be
expected, to a larger number of total instructions per program, and hence a
longer assembly code.

By not supporting various types of complex instructions, RISC processors only
support a limited number of instructions (~200) compared to CISC processors
(~1500). So, to execute complex instructions, this has to be done through a
combination of minor instructions through Assembly.

On the other hand, an advantage of splitting complex instructions into minor
ones is having all instructions of the same length either 32-bit or 64-bit
long. This enables designing the CPU clock speed around the instruction length
so that executing each stage in the instruction cycle would always take
precisely one machine clock cycle.

The below diagram shows how CISC instructions take a variable amount of clock
cycles, while RISC instructions take a fixed amount: risc vs cisc cycles



 \begin{figure}
  \includegraphics[width=\linewidth]{binary/intro/images/assembly_cisc_risk_cycles.jpg}
  \caption{CISC / RISC cycles}
  \label{fig:assembly_cisc_risk_cycles}
\end{figure}



Executing each instruction stage in a single clock cycle and only executing
simple instructions leads to RISC processors consuming a fraction of the power
consumed by CISC processors, which makes these processors ideal for devices
that run on batteries, like smartphones and laptops.

\subsection{CISC vs. RISC}
In the past, having a longer assembly code due to a larger number of total
instructions per program was a significant disadvantage for RISC processors due
to the limited resources in memory and storage. However, today this is no
longer as big of an issue, as memory and storage are not as expensive and
limited as they used to be in the past.

Furthermore, with new assemblers and compilers writing extremely optimized code
on the software level, RISC processors are becoming faster than CISC
processors, even in executing and processing heavy applications, all while
consuming much less power.

All of this is making RISC processors more common in recent years. RISC may
become the dominant architecture in the upcoming years. But as we speak, the
overwhelming majority of computers and servers we will be pentesting are
running on Intel/AMD processors with the CISC architecture, making learning
CISC assembly our priority. As the basics of all Assembly language variants are
pretty similar, learning ARM Assembly should be more straightforward.



