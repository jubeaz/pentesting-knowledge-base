\chapter{Pentest documentation and reporting}

\section{Notetaking}
\subsection{Sample Structure}
\begin{itemize}
    \item {\emph Attack Path} - An outline of the entire path if you gain a
        foothold during an external penetration test or compromise one or more
        hosts (or the AD domain) during an internal penetration test. Outline
        the path as closely as possible using screenshots and command output
        will make it easier to paste into the report later and only need to
        worry about formatting.

    \item {\emph Credentials} - A centralized place to keep your compromised
        credentials and secrets as you go along.

    \item {\emph Findings} - We recommend creating a subfolder for each finding
        and then writing our narrative and saving it in the folder along with
        any evidence (screenshots, command output). It is also worth keeping a
        section in your notetaking tool for recording findings information to
        help organize them for the report.

    \item {\emph Vulnerability Scan Research} - A section to take notes on
        things you've researched and tried with your vulnerability scans (so
        you don't end up redoing work you already did).

    \item {\emph Service Enumeration Research} - A section to take notes on
        which services you've investigated, failed exploitation attempts,
        promising vulnerabilities/misconfigurations, etc.

    \item {\emph Web Application Research} - A section to note down interesting
        web applications found through various methods, such as subdomain
        brute-forcing. It's always good to perform thorough subdomain
        enumeration externally, scan for common web ports on internal
        assessments, and run a tool such as Aquatone or EyeWitness to
        screenshot all applications. As you review the screenshot report, note
        down applications of interest, common/default credential pairs you
        tried, etc.

    \item {\emph AD Enumeration Research} - A section for showing,
        step-by-step, what Active Directory enumeration you've already
        performed. Note down any areas of interest you need to run down later
        in the assessment.

    \item {\emph OSINT} - A section to keep track of interesting information
        you've collected via OSINT, if applicable to the engagement.

    \item {\emph Administrative Information} - Some people may find it helpful
        to have a centralized location to store contact information for other
        project stakeholders like Project Managers (PMs) or client Points of
        Contact (POCs), unique objectives/flags defined in the Rules of
        Engagement (RoE), and other items that you find yourself often
        referencing throughout the project. It can also be used as a running
        to-do list. As ideas pop up for testing that you need to perform or
        want to try but don't have time for, be diligent about writing them
        down here so you can come back to them later.

    \item {\emph Scoping Information} - Here, we can store information about
        in-scope IP addresses/CIDR ranges, web application URLs, and any
        credentials for web applications, VPN, or AD provided by the client. It
        could also include anything else pertinent to the scope of the
        assessment so we don't have to keep re-opening scope information and
        ensure that we don't stray from the scope of the assessment.

    \item {\emph Activity Log} - High-level tracking of everything you did
        during the assessment for possible event correlation.

    \item {\emph Payload Log} - Similar to the activity log, tracking the
        payloads you're using (and a file hash for anything uploaded and the
        upload location) in a client environment is critical. More on this
        later.

\end{itemize}

\subsection{Logging}
It is essential that we log all scanning and attack attempts and keep raw tool
output wherever possible. This will greatly help us come reporting time. Though
our notes should be clear and extensive, we may miss something, and having our
logs to fallback can help us when either adding more evidence to a report or
responding to a client question.

\href{https://github.com/tmux-plugins/tmux-logging}{Tmux logging} is an
excellent choice for terminal logging.

\subsection{Artifacts Left Behind}
At a minimum, we should be tracking when a payload was used, which host it was
used on, what file path it was placed in on the target, and whether it was
cleaned up or needs to be cleaned up by the client. A file hash is also
recommended for ease of searching on the client's part. It's best practice to
provide this information even if we delete any web shells, payloads, or tools.

If we create accounts or modify system settings, it should be evident that we
need to track those things in case we cannot revert them once the assessment is
complete. Some examples of this include:
\begin{itemize}
    \item IP address of the host(s)/hostname(s) where the change was made
    \item Timestamp of the change
    \item Description of the change
    \item Location on the host(s) where the change was made
    \item Name of the application or service that was tampered with
    \item Name of the account (if you created one) and perhaps the password in
        case you are required to surrender it
\end{itemize}

It should go without saying, but as a professional and to prevent creating
enemies out of the infrastructure team, you should get written approval from
the client before making these types of system modifications or doing any sort
of testing that might cause an issue with system stability or availability.
This can typically be ironed out during the project kickoff call to determine
the threshold beyond which the client is willing to tolerate without being
notified.

\subsection{Evidence}

No matter the assessment type, our client (typically) does not care about the
cool exploit chains we pull off or how easily we "pwned" their network.
Ultimately, they are paying for the report deliverable, which should clearly
communicate the issues discovered and evidence that can be used for validation
and reproduction. Without clear evidence, it can be challenging for internal
security teams, sysadmins, devs, etc., to reproduce our work while working to
implement a fix or even to understand the nature of the issue.

\subsubsection{What to Capture}

As we know, each finding will need to have evidence. It may also be prudent to
collect evidence of tests that were performed that were unsuccessful in case
the client questions your thoroughness. If you're working on the command line,
Tmux logs may be sufficient evidence to paste into the report as literal
terminal output, but they can be horribly formatted. For this reason, capturing
your terminal output for significant steps as you go along and tracking that
separately alongside your findings is a good idea. For everything else,
screenshots should be taken.

\subsubsection{Storage}
Much like with our notetaking structure, it's a good idea to come up with a framework for how we organize the data collected during an assessment. This may seem like overkill on smaller assessments, but if we're testing in a large environment and don't have a structured way to keep track of things, we're going to end up forgetting something, violating the rules of engagement, and probably doing things more than once which can be a huge time waster, especially during a time-boxed assessment. Below is a suggested baseline folder structure, but you may need to adapt it accordingly depending on the type of assessment you're performing or unique circumstances.

\begin{itemize}
   \item Admin:    Scope of Work (SoW) that you're working off of, your notes from the project kickoff meeting, status reports, vulnerability notifications, etc
   \item Deliverables:  Folder for keeping your deliverables as you work through them. This will often be your report but can include other items such as supplemental spreadsheets and slide decks, depending on the specific client requirements.
   \item Evidence:
    \begin{itemize}
   \item     Findings:  We suggest creating a folder for each finding you plan to include in the report to keep your evidence for each finding in a container to make piecing the walkthrough together easier when you write the report.
   \item     Scans
        \begin{itemize}
            \item         Vulnerability scans: Export files from your vulnerability scanner (if applicable for the assessment type) for archiving.
            \item         Service Enumeration:  Export files from tools you use to enumerate services in the target environment like Nmap, Massscan, Rumble, etc.
            \item         Web:   Export files for tools such as ZAP or Burp state files, EyeWitness, Aquatone, etc.
            \item         AD Enumeration: JSON files from BloodHound, CSV files generated from PowerView or ADRecon, Ping Castle data, Snaffler log files, CrackMapExec logs, data from Impacket tools, etc.
        \end{itemize}
   \item     Notes: A folder to keep your notes in.
   \item     OSINT:  Any OSINT output from tools like Intelx and Maltego that doesn't fit well in your notes document.
   \item     Wireless: Optional if wireless testing is in scope, you can use this folder for output from wireless testing tools.
   \item     Logging output: Logging output from Tmux, Metasploit, and any other log output that does not fit the Scan subdirectories listed above.
   \item     Misc Files: Web shells, payloads, custom scripts, and any other files generated during the assessment that are relevant to the project.
    \end{itemize}
   \item Retes: This is an optional folder if you need to return after the original assessment and retest the previously discovered findings. You may want to replicate the folder structure you used during the initial assessment in this directory to keep your retest evidence separate from your original evidence.
\end{itemize}

It's a good idea to have scripts and tricks for setting up at the beginning of
an assessment. We could take the following command to make our directories and
subdirectories and adapt it further.

\begin{verbatim}
mkdir -p Folder/{Admin,Deliverables,Evidence/{Findings,Scans/\
    {Vuln,Service,Web,'AD Enumeration'},Notes,OSINT,Wireless,\
    'Logging output','Misc Files'},Retest}
\end{verbatim}

\subsection{Formatting and Redaction}

Credentials and Personal Identifiable Information (PII) should be redacted in
screenshots and anything that would be morally objectionable, like graphic
material or perhaps obscene comments and language.

Wherever possible, we should try to use terminal output over screenshots of the
terminal. It is easier to redact, highlight the important parts (i.e., the
command we ran in blue text and the part of the output we want to call
attention to in red), typically looks neater in the document, and can avoid the
document from becoming a massive, unwieldy file if we have loads of findings.
We should be careful not to alter terminal output since we want to give an
exact representation of the command we ran and the result. It is OK to
shorten/cut out unnecessary output and mark the removed portion with {\bf
<SNIP>} but never alter output or add things that were not in the original
command or output. Using text-based figures also makes it easier for the client
to copy/paste to reproduce your results.

One common way of redacting screenshots is through pixelation or blurring using
a tool such as Greenshot. 

Typically the only thing that needs to be redacted from terminal output is
credentials (whether in the command itself or the output of the command). This
includes password hashes. For password hashes, you can usually just strip out
the middle of them and leave the first and last 3 or 4 characters to show there
was actually a hash there. For cleartext credentials or any other
human-readable content that needs to be obfuscated, you can just replace it
with a {\bf <REDACTED>} or {\bf <PASSWORD REDACTED>} placeholder, or similar.

\subsection{What Not to Archive}

When starting a penetration test, we are being trusted by our customers to
enter their network and "do no harm" wherever possible. This means not bringing
down any hosts or affecting the availability of applications or resources, not
changing passwords (unless explicitly permitted), making significant or
difficult-to-reverse configuration changes, or viewing or removing certain
types of data from the environment. This data may include unredacted PII,
potentially criminal info, anything considered legally "discoverable," etc. For
example, if you gain access to a network share with sensitive data, it's
probably best to just screenshot the directory with the files in it rather than
opening individual files and screenshotting the file contents. If the files are
as sensitive as you think, they'll get the message and know what's in them
based on the file name. Collecting actual PII and extracting it from the target
environment may have significant compliance obligations for storing and
processing that data like GDPR and the like and could open up a slew of issues
for our company and us.

\section{Types of Reports}

\subsection{Draft Report}

It is becoming more commonplace for clients to expect to have a dialogue and
incorporate their feedback into a report. This may come in many forms, whether
they want to add comments about how they plan to address each finding
(management response), tweak potentially inflammatory language, or move things
around to where it suits their needs better. For these reasons, it's best to
plan on submitting a draft report first, giving the client time to review it on
their own, and then offering a time slot where they can review it with you to
ask questions, get clarification, or explain what they would like to see. The
client is paying for the report deliverable in the end, and we must ensure it
is as thorough and valuable to them as possible. Some will not comment on the
report at all, while others will ask for significant changes/additions to help
it suit their needs, whether it be to make it presentable to their board of
directors for additional funding or use the report as an input to their
security roadmap for performing remediation and hardening their security
posture.

\subsection{Final Report}

Typically, after reviewing the report with the client and confirming that they
are satisfied with it, you can issue the final report with any necessary
modifications. This may seem like a frivolous process, but several auditing
firms will not accept a draft report to fulfill their compliance obligations,
so it's important from the client's perspective.

\subsection{Post-Remediation Report}

It is also common for a client to request that the findings you discovered
during the original assessment be tested again after they've had an opportunity
to correct them. This is all but required for organizations beholden to a
compliance standard such as PCI. You should not be redoing the entire
assessment for this phase of the assessment. But instead, you should be
focusing on retesting only the findings and only the hosts affected by those
findings from the original assessment. You also want to ensure that there is a
time limit on how long after the initial assessment we perform remediation
testing. Here are some of the things that might happen if you don't.

\begin{itemize}
   \item  The client asks you to test their remediation several months or even
       a year or more later, and the environment has changed so much that it's
       impossible to get an "apples to apples" comparison.

   \item  If you check the entire environment for new hosts affected by a given
       finding, you may discover new hosts that are affected and fall into an
       endless loop of remediation testing the new hosts you discovered last
       time.

   \item  If you run new large-scale scans like vulnerability scans, you will
       likely find stuff that wasn't there before, and your scope will quickly
       get out of control.

   \item  If a client has a problem with the "snapshot" nature of this type of
       testing, you could recommend a Breach and Attack Simulation (BAS) type
       tool to periodically run those scenarios to ensure they do not continue
       popping up.
\end{itemize}

If any of these situations occur, you should expect more scrutiny around
severity levels and perhaps pressure to modify things that should not be
modified to help them out. In these situations, your response should be
carefully crafted to be both clear that you’re not going to cross ethical
boundaries (but be careful about insinuating that they’re asking you to do
something intentionally dishonest, indicating that they are dishonest), but
also commiserate with their situation and offer some ways out of it for them.
For example, if their concern is being on the hook with an auditor to fix
something in an amount of time that they don’t have, they may be unaware that
many auditors will accept a thoroughly documented remediation plan with a
reasonable deadline on it (and justification for why it cannot be completed
more quickly) instead of remediating and closing the finding within the
examination period. This allows you to keep your integrity intact, fosters the
feeling with the client that you sincerely care about their plight, and gives
them a path forward without having to turn themselves inside out to make it
happen.

One approach could be to treat this as a new assessment in these situations. If
the client is unwilling, then we would likely want to retest just the findings
from the original report and carefully note in the report the length of time
that has passed since the original assessment, that this is a point in time
check to assess whether ONLY the previously reported vulnerabilities affect the
originally reported host or hosts and that it's likely the client's environment
has changed significantly, and a new assessment was not performed.

In terms of report layout, some folks may prefer to update the original
assessment by tagging affected hosts in each finding with a status (e.g.,
resolved, unresolved, partial, etc.), while others may prefer to issue a new
report entirely that has some additional comparison content and an updated
executive summary.

\subsection{Attestation Report}

Some clients will request an Attestation Letter or Attestation Report that is
suitable for their vendors or customers who require evidence that they've had a
penetration test done. The most significant difference is that your client will
not want to hand over the specific technical details of all of the findings or
credentials or other secret information that may be included to a third party.
This document can be derived from the report. It should focus only on the
number of findings discovered, the approach taken, and general comments about
the environment itself. This document should likely only be a page or two
long.

\subsection{Other Deliverables}
\subsubsection{Slide Deck}

You may also be requested to prepare a presentation that can be given at
several different levels. Your audience may be technical, or they may be more
executive. The language and focus should be as different in your executive
presentation as the executive summary is from the technical finding details in
your report. Only including graphs and numbers will put your audience to sleep,
so it's best to be prepared with some anecdotes from your own experience or
perhaps some recent current events that correlate to a specific attack vector
or compromise. Bonus points if said story is in the same industry as your
client. The purpose of this is not fear-mongering, and you should be careful
not to present it that way, but it will help hold your audience's attention. It
will make the risk relatable enough to maximize their chances of doing
something about it.

\subsubsection{Spreadsheet of Findings}

The spreadsheet of findings should be pretty self-explanatory. This is all of
the fields in the findings of your report, just in a tabular layout that the
client can use for easier sorting and other data manipulation. This may also
assist them with importing those findings into a ticketing system for internal
tracking purposes. This document should not include your executive summary or
narratives. Ideally, learn how to use pivot tables and use them to create some
interesting analytics that the client might find interesting. The most helpful
objective in doing this is sorting findings by severity or category to help
prioritize remediation.

\subsection{Vulnerability Notifications}

Sometimes during an assessment, we will unover a critical flaw that requires us
to stop work and inform our clients of an issue so they can decide if they
would like to issue an emergency fix or wait until after the assessment is
over.


At a minimum, this should be done for any finding that is directly exploitable that is exposed to the internet and results in unauthenticated remote code execution or sensitive data exposure, or leverage weak/default credentials for the same. Beyond that, expectations should be set for this during the project kickoff process. Some clients may want all high and critical findings reported out-of-band regardless of whether they're internal or external. Some folks may need mediums as well. It's usually best to set a baseline for yourself, tell the client what to expect, and let them ask for modifications to the process if they need them.

Due to the nature of these notifications, it's important to limit the amount of
fluff in these documents so the technical folks can get right to the details
and begin fixing the issue. For this reason, it's probably best to limit this
to the typical content you have in the technical details of your findings and
provide tool-based evidence for the finding that the client can quickly
reproduce if needed.

\section{Components of a Report}
As mentioned previously, the report is the main deliverable that a client is
paying for when they contract your firm to perform a penetration test. The
report is our chance to show off our work during the assessment and provide the
client with as much value as possible. Ideally, the report will be free of
extraneous data and information that "clutter" up the report or distract from
the issues we are trying to convey of the overall picture of their security
posture we are trying to paint. Everything in the report should have a reason
for being there, and we don't want to overwhelm the reader (for example, don't
paste in 50+ pages of console output!). In this section, we'll cover the key
elements of a report and how we can best structure it to show off our work and
help our clients prioritize remediation.

\subsection{Prioritizing Our Efforts}

During an assessment, especially large ones, we'll be faced with a lot of
"noise" that we need to filter out to best focus our efforts and prioritize
findings. As testers, we are required to disclose everything we find, but when
there is a ton of information coming at us through scans and enumeration, it is
easy to get lost or focus on the wrong things and waste time and potentially
miss high-impact issues. This is why it is essential that we understand the
output that our tools produce, have repeatable steps (such as scripts or other
tools) to sift through all of this data, process it, and remove false positives
or informational issues that could distract us from the goal of the assessment.
Experience and a repeatable process are key so that we can sift through all of
our data and focus our efforts on high-impact findings such as remote code
execution (RCE) flaws or others that may lead to sensitive data disclosure. It
is worth it (and our duty) to report informational findings, but instead of
spending the majority of our time validating these minor, non-exploitable
issues, you may want to consider consolidating some of them into categories
that show the client you were aware that the issues existed, but you were
unable to exploit them in any meaningful way (e.g., 35 different variations of
problems with SSL/TLS, a ton of DoS vulnerabilities in an EOL version of PHP,
etc.).

When starting in penetration testing, it can be difficult to know what to
prioritize, and we may fall down rabbit holes trying to exploit a flaw that
doesn't exist or getting a broken PoC exploit to work. Time and experience help
here, but we should also lean on senior team members and mentors to help.
Something that you may waste half a day on could be something that they have
seen many times and could tell you quickly whether it is a false positive or
worth running down. Even if they can't give you a really quick black and white
answer, they can at least point you in a direction that saves you several
hours. Surround yourself with people you're comfortable with asking for help
that won't make you feel like an idiot if you don't know all the answers.

\subsection{Writing an Attack Chain}

The attack chain is our chance to show off the cool exploitation chain we took
to gain a foothold, move laterally, and compromise the domain. It can be a
helpful mechanism to help the reader connect the dots when multiple findings
are used in conjunction with each other and gain a better understanding of why
certain findings are given the severity rating that they are assigned. For
example, a particular finding on its own may be medium-risk but, combined with
one or two other issues, could elevate it to high-risk, and this section is our
chance to demonstrate that. A common example is using Responder to intercept
NBT-NS/LLMNR traffic and relaying it to hosts where SMB signing is not present.
It can get really interesting if some findings can be incorporated that might
otherwise seem inconsequential, like using an information disclosure of some
sort to help guide you through an LFI to read an interesting configuration
file, log in to an external-facing application, and leverage functionality to
gain remote code execution and a foothold inside the internal network.

There are multiple ways to present this, and your style may differ but let's
walk through an example. We will start with a summary of the attack chain and
then walk through each step along with supporting command output and
screenshots to show the attack chain as clearly as possible. A bonus here is
that we can re-use this as evidence for our individual findings so we don't
have to format things twice and can copy/paste them into the relevant finding.

\subsubsection{Sample Attack Chain - INLANEFREIGHT.LOCAL Internal Penetration
Test}

During the Internal Penetration Test performed against Inlanefreight, the
tester gained a foothold in the internal network, moved laterally, and
ultimately compromised the INLANEFREIGHT.LOCAL Active Directory domain. The
below walkthrough illustrates the steps taken to go from an unauthenticated
anonymous user in the internal network to Domain Admin level access. The intent
of this attack chain is to demonstrate to Inlanefreight the impact of each
vulnerability shown in this report and how they fit together to demonstrate the
overall risk to the client environment and help to prioritize remediation
efforts (i.e., patching two flaws quickly could break up the attack chain while
the company works to remediate all issues reported). While other findings shown
in this report could be leveraged to gain a similar level of access, this
attack chain shows the initial path of least resistance taken by the tester to
achieve domain compromise.

\begin{enumerate}
   \item  The tester utilized the Responder tool to obtain an NTLMv2 password hash for a domain user, bsmith.

   \item  This password hash was successfully cracked offline using the Hashcat tool to reveal the user's cleartext password, which granted a foothold into the INLANEFREIGHT.LOCAL domain, but with no more privileges than a standard domain user.

   \item  The tester then ran the BloodHound.py tool, a Python version of the popular SharpHound collection tool to enumerate the domain and create visual representations of attack paths. Upon review, the tester found that multiple privileged users existed in the domain configured with Service Principal Names (SPNs), which can be leveraged to perform a Kerberoasting attack and retrieve TGS Kerberos tickets for the accounts which can be cracked offline using Hashcat if a weak password is set. From here, the tester used the GetUserSPNs.py tool to carry out a targeted Kerberoasting attack against the mssqlsvc account, having found that the mssqlsvc account had local administrator rights over the host SQL01.INLANEFREIGHT.LOCAL which was an interesting target in the domain.

   \item  The tester successfully cracked this account's password offline, revealing the cleartext value.

   \item  The tester authenticated to the host SQL01.INLANEFREIGHT.LOCAL and retrieved a cleartext password from the host's registry by decrypting LSA secrets for an account (srvadmin), which was set up for autologon.

   \item  This srvadmin account had local administrator rights over all servers (aside from Domain Controllers) in the domain, so the tester logged into the MS01.INLANEFREIGHT.LOCAL host and retrieved a Keberos TGT ticket for a logged-in user, pramirez This user was part of the Tier I Server Admins group, which granted the account DCSync rights over the domain object. This attack can be utilized to retrieve the NTLM password hash for any user in the domain, resulting in domain compromise and persistence via a Golden Ticket.

   \item  The tester used the Rubeus tool to extract the Kerberos TGT ticket for the pramirez user and perform a Pass-the-Ticket attack to authenticate as this user.

   \item  Finally, the tester performed a DCSync attack after successfully authenticating with this user account via the Mimikatz tool, which ended in domain compromise.

\end{enumerate}

\subsubsection{Detailed reproduction steps for this attack chain are as
follows:}

Upon connecting to the network, the tester started the Responder tool and was able to capture a password hash for the bsmith user by spoofing NBT-NS/LLMNR traffic on the local network segment.
{\bf Responder}
\begin{verbatim}
jubeaz@htb[/htb]$  sudo responder -I eth0 -wrfv

                                         __
  .----.-----.-----.-----.-----.-----.--|  |.-----.----.
  |   _|  -__|__ --|  _  |  _  |     |  _  ||  -__|   _|
  |__| |_____|_____|   __|_____|__|__|_____||_____|__|
                   |__|

           NBT-NS, LLMNR & MDNS Responder 3.0.6.0

 <SNIP>

[+] Generic Options:
    Responder NIC              [eth0]
    Responder IP               [192.168.195.168]
    Challenge set              [random]
    Don't Respond To Names     ['ISATAP']

[+] Current Session Variables:
    Responder Machine Name     [WIN-TWWXTGD94CV]
    Responder Domain Name      [3BKZ.LOCAL]
    Responder DCE-RPC Port     [47032]

[+] Listening for events...

<SNIP>

[SMB] NTLMv2-SSP Client   : 192.168.195.205
[SMB] NTLMv2-SSP Username : INLANEFREIGHT\bsmith
[SMB] NTLMv2-SSP Hash     : bsmith::INLANEFREIGHT:7ecXXXXXX98ebc:73D1B2XXXXXXXXXXX45085A651:010100000000000000B588D9F766D801191BB2236A5FAAA50000000002000800330042004B005A0001001E00570049004E002D005400570057005800540047004400390034004300560004003400570049004E002D00540057005700580054004700440039003400430056002E00330042004B005A002E004CXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX2E004C004F00430041004C000700080000B588D9F766D801060004000200000008003000300000000000000001000000002000002CAE5BF3BB1FD2F846A280AEF43A8809C15207BFCB4DF5A580BA1B6FCAF6BBCE0A001000000000000000000000000000000000000900280063006900660073002F003100390032002E003100360038002E003100390035002E00310036003800000000000000000000000000

<SNIP>
\end{verbatim}

The tester was successful in "cracking" this password hash offline using the Hashcat tool and retrieving the cleartext password value, thus granting a foothold to enumerate the Active Directory domain.
{\bf Hashcat}

\ldots

\subsection{Writing a Strong Executive Summary}

The Executive Summary is one of the most important parts of the report. As
mentioned previously, our clients are ultimately paying for the report
deliverable which has several purposes aside from showing weaknesses and
reproduction steps that can be used by technical teams working on remediation.
The report will likely be viewed in some part by other internal stakeholders
such as Internal Audit, IT and IT Security management, C-level management, and
even the Board of Directors. The report may be used to either validate funding
from the prior year for infosec or to request additional funding for the
following year. For this reason, we need to ensure that there is content in the
report that can be easily understood by people without technical knowledge.

\subsubsection{Key concepts}
The intended audience for the Executive Summary is typically the person that is going to be responsible for allocating the budget to fixing the issues we discovered. For better or worse, some of our clients have likely been trying to get funding to fix the issues presented in the report for years and fully intend to use the report as ammunition to finally get some stuff done. This is our best chance to help them out. If we lose our audience here and there are budgetary limitations, the rest of the report can quickly become worthless. Some key things to assume (that may or may not be true) to maximize the effectiveness of the Executive Summary are:

\begin{itemize}
   \item  It should be obvious, but this should be written for someone who isn't technical at all. The typical barometer for this is "if your parents can't understand what the point is, then you need to try again" (assuming your parents aren't CISOs or sysadmins or something of the sort).

   \item  The reader doesn't do this every day. They don't know what Rubeus does, what password spraying means, or how it's possible that tickets can grant different tickets (or likely even what a ticket is, aside from a piece of paper to enter a concert or a ballgame).

   \item  This may be the first time they've ever been through a penetration test.

   \item  Much like the rest of the world in the instant gratification age, their attention span is small. When we lose it, we are extraordinarily unlikely to get it back.

   \item  Along the same lines, no one likes to read something where they have to Google what things mean. Those are called distractions.
\end{itemize}

\subsubsection{Do and don't}
{\bf Do}


\begin{itemize}
   \item  When talking about metrics, be as specific as possible. - Words like "several," "multiple," and "few" are ambiguous and could mean 6 or 500. Executives aren't going to dig through the report for this information, so if you're going to talk about this, let them know what you've got; otherwise, you're going to lose their attention. The most common reason people do not commit to a specific number is to leave it open in case the consultant missed one. You can make minor changes to the language to account for this, such as "while there may be additional instances of X, in the time allotted to the assessment, we observed 25 occurrences of X".

   \item  It's a summary. Keep it that way. - If you wrote more than 1.5-2 pages, you've probably been too verbose. Examine the topics you talked about and determine whether they can be collapsed into higher-level categories that might fall into specific policies or procedures.

   \item  Describe the types of things you managed to access - Your audience may not have any idea what "Domain Admin" means, but if you mention that you gained access to an account that enabled you to get your hands on HR documents, banking systems, and other critical assets, that's universally understandable.

   \item  Describe the general things that need to improve to mitigate the risks you discovered. - This should not be "install 3 patches and call me in a year". You should be thinking in terms of "what process broke down that enabled a five-year-old vulnerability to go unpatched in a quarter of the environment?". If you password spray and get 500 hits on Welcome1!, changing the passwords of those 500 accounts is only part of the solution. The other part is probably providing the Help Desk with a way to set stronger initial passwords efficiently.

   \item  If you're feeling brave and have a decent amount of experience on both sides, provide a general expectation for how much effort will be necessary to fix some of this. - If you have a long past as a sysadmin or engineer and you know how much internal politics people may have to wade through to start manipulating group policies, you may want to try and set an expectation for low, moderate, and significant levels of time and effort to correct the issues, so an overzealous CEO doesn't go tell his server team they need to apply CIS hardening templates to their GPOs over the weekend without testing them first.
\end{itemize}


{\bf Do not}
\begin{itemize}
   \item  Name or recommend specific vendors. - The deliverable is a technical document, not a sales document. It's acceptable to suggest technologies such as EDR or log aggregation but stay away from recommending specific vendors of those technologies, like CrowdStrike and Splunk. If you have experience with a particular vendor that is recent and you feel comfortable giving the client that feedback, do so out-of-band and make sure that you're clear that they should make their own decision (and probably bring the client's account executive into that discussion). If you're describing specific vulnerabilities, your reader is more likely to recognize something like "vendors like VMWare, Apache, and Adobe" instead of "vSphere, Tomcat, and Acrobat."

   \item  Use Acronyms. - IP and VPN have reached a level of ubiquity that they're maybe okay, but using acronyms for protocols and types of attacks (e.g., SNMP, MitM) is tone-deaf and will render your executive summary completely ineffective for its intended audience.

   \item  Spend more time talking about stuff that doesn't matter than you do about the significant findings in the report. - It is within your power to steer attention. Don't waste it on the issues you discovered that weren't that impactful.

   \item  Use words that no one has ever heard of before. - Having a large vocabulary is great, but if no one can understand the point you're trying to make or they have to look up what words mean, all they are is a distraction. Show that off somewhere else.

   \item  Reference a more technical section of the report. - The reason the executive is reading this might be because they don't understand the technical details, or they may decide they just don't have time for it. Also, no one likes having to scroll back and forth throughout the report to figure out what's going on.
\end{itemize}

\subsubsection{Vocabulary Changes}

To provide some examples of what it means to "write to a non-technical audience," we've provided some examples below of technical terms and acronyms you may be tempted to use, along with a less technical alternative that could be used instead. This list is not exhaustive nor the "right" way to describe these things. They are meant as examples of how you might describe a technical topic in a more universally understandable way.

\begin{itemize}
        \item VPN, SSH - a protocol used for secure remote administration
        \item SSL/TLS - technology used to facilitate secure web browsing
        \item Hash - the output from an algorithm commonly used to validate file integrity
        \item Password Spraying - an attack in which a single, easily-guessable password is attempted for a large list of harvested user accounts
        \item Password Cracking - an offline password attack in which the cryptographic form of a user’s password is converted back to its human-readable form
        \item Buffer overflow/deserialization/etc. - an attack that resulted in remote command execution on the target host
        \item OSINT - Open Source Intelligence Gathering, or hunting/using data about a company and its employees that can be found using search engines and other public sources without interacting with a company's external network
        \item SQL injection/XSS - a vulnerability in which input is accepted from the user without sanitizing characters meant to manipulate the application's logic in an unintended manner
\end{itemize}
These are just a few examples. Your glossary will grow over time as you write
more reports. You can also improve this in this area by reading the executive
summaries that others have written describing some of the same findings that
you typically discover. Doing so can be the catalyst for thinking of something
in a different way. You may also receive feedback from the client from time to
time about this, and it is important to receive this feedback gracefully and
with an open mind. You may be tempted to get defensive (especially if the
client is being really aggressive), but at the end of the day, they paid you to
build them a useful product. If it isn't because they can't understand it, then
look at it as an opportunity to practice and grow. Taking client feedback as a
personal attack may be difficult not to do, but it's one of the most valuable
things they can give you.

\subsubsection{Summary of Recommendations}

Before we get into the technical findings, it's a good idea to provide a
Summary of Recommendations or Remediation Summary section. Here we can list our
short, medium, and long-term recommendations based on our findings and the
current state of the client's environment. We'll need to use our experience and
knowledge of the client's business, security budget, staffing considerations,
etc., to make accurate recommendations. Our clients will often have input on
this section, so we want to get it right, or the recommendations are useless.
If we structure this properly, our clients can use it as the basis for a
remediation roadmap. If you opt not to do this, be prepared for clients to ask
you to prioritize remediation for them. It may not happen all the time, but if
you have a report with 15 high-risk findings and nothing else, they're likely
going to want to know which of them is "the most high." As the saying goes,
"when everything is important, nothing is important."

We should tie each recommendation back to a specific finding and not include
any short or medium-term recommendations that are not actionable by remediating
findings reported later in the report. Long-term recommendations may map back
to informational/best practice recommendations such as "Create baseline
security templates for Windows Server and Workstation hosts" but may also be
catch-all recommendations such as "Perform periodic Social Engineering
engagements with follow-on debriefings and security awareness training to build
a security-focused culture within the organization from the top down."

Some findings could have an associated short and long-term recommendation. For
example, if a particular patch is missing in many places, that is a sign that
the organization struggles with patch management and perhaps does not have a
strong patch management program, along with associated policies and procedures.
The short-term solution would be to push out the relevant patches, while the
long-term objective would be to review patch and vulnerability management
processes to address any gaps that would prevent the same issue from cropping
up again. In the application security world, it might instead be fixing the
code in the short term and in the long term, reviewing the SDLC to ensure
security is considered early enough in the development process to prevent these
issues from making it into production.

\subsubsection{Findings}

After the Executive Summary, the Findings section is one of the most important.
This section gives us a chance to show off our work, paint the client a picture
of the risk to their environment, give technical teams the evidence to validate
and reproduce issues and provide remediation advice. We will discuss this
section of the report in detail in the next section of this module: How to
Write up a Finding.

\subsubsection{Appendices}

There are appendices that should appear in every report, but others will be
dynamic and may not be necessary for all reports. If any of these appendices
bloat the size of the report unnecessarily, you may want to consider whether a
supplemental spreadsheet would be a better way to present the data (not to
mention the enhanced ability to sort and filter).

\subsubsection{Static Appendices}
{\bf Scope}

Shows the scope of the assessment (URLs, network ranges, facilities, etc.).
Most auditors that the client has to hand your report to will need to see
this.

{\bf Methodology}

Explain the repeatable process you follow to ensure that your assessments are
thorough and consistent.

{\bf Severity Ratings}

If your severity ratings don't directly map to a CVSS score or something
similar, you will need to articulate the criteria necessary to meet your
severity definitions. You will have to defend this occasionally, so make sure
it is sound and can be backed up with logic and that the findings you include
in your report are rated accordingly.

{\bf Biographies}

If you perform assessments with the intent of fulfilling PCI compliance
specifically, the report should include a bio about the personnel performing
the assessment with the specific goal of articulating that the consultant is
adequately qualified to perform the assessment. Even without compliance
obligations, it can help give the client peace of mind that the person doing
their assessment knew what they were doing.

\subsubsection{Dynamic Appendices}
{\bf Exploitation Attempts and Payloads}

If you've ever done anything in incident response, you should know how many
artifacts are left behind after a penetration test for the forensics guys to
try and sift through. Be respectful and keep track of the stuff you did so that
if they experience an incident, they can differentiate what was you versus an
actual attacker. If you generate custom payloads, particularly if you drop them
on disk, you should also include the details of those payloads here, so the
client knows exactly where to go and what to look for to get rid of them. This
is especially important for payloads that you cannot clean up yourself.

{\bf Compromised Credentials}

If a large number of accounts were compromised, it is helpful to list them here
(if you compromise the entire domain, it might be a wasted effort to list out
every user account instead of just saying "all domain accounts") so that the
client can take action against them if necessary.

{\bf Configuration Changes}

If you made any configuration changes in the client environment (hopefully you
asked first), you should itemize all of them so that the client can revert them
and eliminate any risks you introduced into the environment (like disabling EDR
or something). Obviously, it's ideal if you put things back the way you found
them yourself and get approval in writing from the client to change things to
prevent getting yelled at later on if your change has unintended consequences
for a revenue-generating process.

{\bf Additional Affected Scope}

If you have a finding with a list of affected hosts that would be too much to
include with the finding itself, you can usually reference an appendix in the
finding to see a complete list of the affected hosts where you can create a
table to display them in multiple columns. This helps keep the report clean
instead of having a bulleted list several pages long.

{\bf Information Gathering}

If the assessment is an External Penetration test, we may include additional
data to help the client understand their external footprint. This could include
whois data, domain ownership information, subdomains, discovered emails,
accounts found in public breach data (DeHashed is great for this), an analysis
of the client's SSL/TLS configurations, and even a listing of externally
accessible ports/services (in a large scope external you'd likely want to make
a supplementary spreadsheet). This data can be beneficial in a
low-to-no-finding report but should convey some sort of value to the client and
not just be "fluff."

{\bf Domain Password Analysis}

If you're able to gain Domain Admin access and dump the NTDS database, it's a
good idea to run this through Hashcat with multiple wordlists and rules and
even brute-force NTLM up through eight characters if your password cracking rig
is powerful enough. Once you've exhausted your cracking attempts, a tool such
as DPAT can be used to produce a nice report with various statistics. You may
want just to include some key stats from this report (i.e., number of hashes
obtained, number and percentage cracked, number of privileged accounts cracks
(think Domain Admins and Enterprise Admins), top X passwords, and the number of
passwords cracked for each character length). This can help drive home themes
in the Executive Summary and Findings sections regarding weak passwords. You
may also wish to provide the client with the entire DPAT report as
supplementary data.

\section{How to Write Up a Finding}

he Findings section of our report is the "meat." This is where we get to show
off what we found, how we exploited them, and give the client guidance on how
to remediate the issues. The more detail we can put into each finding, the
better. This will help technical teams reproduce the finding on their own and
then be able to test that their fix worked. Being detailed in this section will
also help whoever is tasked with the post-remediation assessment if the client
contracts your firm to perform it. While we'll often have "stock" findings in
some sort of database, it's essential to tweak them to fit our client's
environment to ensure we aren't mispresenting anything.

\subsection{Breakdown of a Finding}

Each finding should have the same general type of information that should be
customized to your client's specific circumstances. If a finding is written to
suit several different scenarios or protocols, the final version should be
adjusted to only reference the particular circumstances you identified.
"Default Credentials" could have different meanings for risk if it affects a
DeskJet printer versus the building's HVAC control or another high-impact web
application. At a minimum, the following information should be included for
each finding:

\begin{itemize}
   \item Description of the finding and what platform(s) the vulnerability affects
   \item Impact if the finding is left unresolved
   \item Affected systems, networks, environments, or applications
   \item Recommendation for how to address the problem
   \item Reference links with additional information about the finding and resolving it
   \item Steps to reproduce the issue and the evidence that you collected
\end{itemize}

Some additional, optional fields include:

\begin{itemize}
   \item  CVE
   \item  OWASP, MITRE IDs
   \item  CVSS or similar score
   \item  Ease of exploitation and probability of attack
   \item  Any other information that might help learn about and mitigate the attack
\end{itemize}

i\subsection{Showing Finding Reproduction Steps Adequately}

As mentioned in the previous section regarding the Executive Summary, it's
important to remember that even though your point-of-contact might be
reasonably technical, if they don't have a background specifically in
penetration testing, there is a pretty decent chance they won't have any idea
what they're looking at. They may have never even heard of the tool you used to
exploit the vulnerability, much less understand what's important in the wall of
text it spits out when the command runs. For this reason, it's crucial to guard
yourself against taking things for granted and assuming people know how to fill
in the blanks themselves. If you don't do this correctly, again, this will
erode the effectiveness of your deliverable, but this time in the eyes of your
technical audience. Some concepts to consider:
\begin{itemize}

   \item  Break each step into its own figure. If you perform multiple steps in the same figure, a reader unfamiliar with the tools being used may not understand what is taking place, much less have an idea of how to reproduce it themselves.

   \item  If setup is required (e.g., Metasploit modules), capture the full configuration so the reader can see what the exploit config should look like before running the exploit. Create a second figure that shows what happens when you run the exploit.

   \item  Write a narrative between figures describing what is happening and what is going through your head at this point in the assessment. Do not try to explain what is happening in the figure with the caption and have a bunch of consecutive figures.

   \item  After walking through your demonstration using your preferred toolkit, offer alternative tools that can be used to validate the finding if they exist (just mention the tool and provide a reference link, don't do the exploit twice with more than one tool).
\end{itemize}

Your primary objective should be to present evidence in a way that is
understandable and actionable to the client. Think about how the client will
use the information you're presenting. If you're showing a vulnerability in a
web application, a screenshot of Burp isn't the best way to present this
information if you're crafting your own web requests. The client will probably
want to copy/paste the payload from your testing to recreate it, and they can't
do that if it's just a screenshot.

Another critical thing to consider is whether your evidence is completely and
utterly defensible. For example, if you're trying to demonstrate that
information is being transmitted in clear text because of the use of basic
authentication in a web application, it's insufficient just to screenshot the
login prompt popup. That shows that basic auth is in place but offers no proof
that information is being transmitted in the clear. In this instance, showing
the login prompt with some fake credentials entered into it, and the clear text
credentials in a Wireshark packet capture of the human-readable authentication
request leaves no room for debate. Similarly, if you're trying to demonstrate
the presence of a vulnerability in a particular web application or something
else with a GUI (like RDP), it's important to capture either the URL in the
address bar or output from an ifconfig or ipconfig command to prove that it's
on the client's host and not some random image you downloaded from Google.
Also, if you're screenshotting your browser, turn your bookmarks bar off and
disable any unprofessional browser extensions or dedicate a specific web
browser to your testing.


\subsection{Effective Remediation Recommendations}
\subsubsection{Example 1}

\begin{itemize}
    \item Bad: Reconfigure your registry settings to harden against X.

    \item Good: To fully remediate this finding, the following registry hives should be updated with the specified values. Note that changes to critical components like the registry should be approached with caution and tested in a small group prior to making large-scale changes.
\begin{itemize}
    \item [list the full path to the affected registry hives]
\begin{itemize}
    \item   Change value X to value Y
\end{itemize}
\end{itemize}
\end{itemize}

While the "bad" example is at least somewhat helpful, it's fairly lazy, and you're squandering a learning opportunity. Once again, the reader of this report may not have the depth of experience in Windows as you, and giving them a recommendation that will require hours' worth of work for them to figure out how to do it is only going to frustrate them. Do your homework and be as specific as reasonably possible. Doing so has the following benefits:


\begin{itemize}
    \item  You learn more this way and will be much more comfortable answering questions during the report review. This will reinforce the client's confidence in you and will be knowledge that you can leverage on future assessments and to help level up your team.

    \item The client will appreciate you doing the research for them and outlining specifically what needs to be done so they can be as efficient as possible. This will increase the likelihood that they will ask you to do future assessments and recommend you and your team to their friends.
\end{itemize}

It's also worth drawing attention to the fact that the "good" example includes
a warning that changing something as important as the registry carries its own
set of risks and should be performed with caution. Again, this indicates to the
client that you have their best interests in mind and genuinely want them to
succeed. For better or worse, there will be clients that will blindly do
whatever you tell them to and will not hesitate to try and hold you accountable
if doing so ends up breaking something.

\subsubsection{Example 2}

\begin{itemize}
    \item Bad: Implement [some commercial tool that costs a fortune] to address this finding.

    \item Good: There are different approaches to addressing this finding. [Name of the affected software vendor] has published a workaround as an interim solution. For the sake of brevity, a link to the walkthrough has been provided in the reference links below. Alternatively, there are commercial tools available that would make it possible to disable the vulnerable functionality in the affected software altogether, but these tools may be cost-prohibitive.
\end{itemize}

The "bad" example gives the client no way to remediate this issue without
spending a lot of money that they may not have. While the commercial tool may
be the easiest solution far and away, many clients will not have the budget to
do that and need an alternative solution. The alternative solution may be a
bandaid or extraordinarily cumbersome, or both, but it will at least buy the
client some time until the vendor has released an official fix.

\subsection{Selecting Quality References}

Each finding should include one or more external references for further reading
on a particular vulnerability or misconfiguration.  Don't choose articles behind a paywall or something where you only get part of what you need without paying.

\begin{itemize}
\item     Use articles that get to the point quickly. This isn't a recipe website, and no one cares how often your grandmother used to make those cookies. We have problems to solve, and making someone dig through the entire NIST 800-53 document or an RFC is more annoying than helpful.
\item 
\item     Choose sources that have clean websites and don't make you feel like a bunch of crypto miners are running in the background or ads pop up everywhere.
\item 
\item     If possible, write some of your own source material and blog about it. The research will aid you in explaining the impact of the finding to your clients, and while the infosec community is pretty helpful, it'd be preferable not to send your clients to a competitor's website.
\end{itemize}





\section{links}

\href{https://github.com/blacklanternsecurity/writehat}{WriteHat}

\href{https://github.com/pwndoc/pwndoc}{Pwndoc}


\href{https://github.com/juliocesarfort/public-pentesting-reports}{https://github.com/juliocesarfort/public-pentesting-reports}

\url{https://www.blackhillsinfosec.com/how-to-not-suck-at-reporting-or-how-to-write-great-pentesting-reports/}
